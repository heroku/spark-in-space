#!/usr/bin/env bash
# bin/compile <build-dir> <cache-dir> <env-dir>
# heroku inline buildpack

set -e

build_dir=$1
cache_dir=$2
env_dir=$3
bp_dir=$(dirname $(dirname $0))

notice() {
  echo "-----> $@..."
}

start() {
  echo -n "-----> $@..."
}

finish() {
  echo " Done."
}

cd $build_dir

spark_version=1.6.1
hadoop_version=2.6
hadoop_aws_shade_27=hadoop-aws-shaded-no-jackson-0.2-hadoop-2.7-SNAPSHOT-shaded.jar
hadoop_aws_shade_26=hadoop-aws-shaded-no-jackson-0.1-SNAPSHOT-shaded.jar
hadoop_aws_shade=$hadoop_aws_shade_26

if [ -f "$build_dir/.spark.version" ]; then
    spark_version=$(cat "$build_dir/.spark.version" | xargs)
    notice "Detected spark version: ${spark_version} configured in .spark.version"
    if [ "$spark_version" == "2.0.0" ]; then
      hadoop_version=2.7
      hadoop_aws_shade=$hadoop_aws_shade_27
    elif [ "$spark_version" == "2.1.0" ]; then
          hadoop_version=2.7
          hadoop_aws_shade=$hadoop_aws_shade_27
    elif [ "$spark_version" == "2.1.1" ]; then
          hadoop_version=2.7
          hadoop_aws_shade=$hadoop_aws_shade_27
    elif [ "$spark_version" == "2.0.2" ]; then
          hadoop_version=2.7
          hadoop_aws_shade=$hadoop_aws_shade_27
    elif [ "$spark_version" == "1.6.1" ]; then
      hadoop_version=2.6
    elif [ "$spark_version" == "1.6.2_scala_2.11" ]; then
      hadoop_version=2.6
      spark_version=1.6.2_scala_2.11
    elif [ "$spark_version" == "1.4.1" ]; then
      hadoop_version=2.6
    else
      notice "Only 1.4.1, 1.6.1, 1.6.2_scala_2.11, 2.0.0, 2.0.2, 2.1.0, 2.1.1 are currently supported as .spark.version, exiting."
      exit 1
    fi
else
  notice "Defaulting to Spark 1.6.1 for Hadoop 2.6"
fi

fetch_spark_tarball() {
    local tarball_file="spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local stack="cedar-14"
    local spark_tarball_url="https://s3-external-1.amazonaws.com/heroku-spark/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local dest_path="$cache_dir/$stack/$tarball_file"

    if [ -f "$dest_path" ]; then
        echo -n "cat $dest_path"
    else
        echo -n "curl -s -L $spark_tarball_url > $dest_path && cat $dest_path"
    fi
}

# Copy a file from the buildpack to the same relative location in the
# build directory, unless it already exists.
# Optional second arg is the namespace/prefix to contain the target within the base.
cautious_build_overlay() {
  local target=$1
  local dest_namespace="${2-}"
  local source_base=$bp_dir
  local dest_base=$build_dir
  if [ -f "${dest_base}/${dest_namespace}${target}" ]
  then
    echo "-----> Skipping copy, already exists in build: ${target}"
  else
    echo "-----> Copying into build: ${target}"
    mkdir -p "${dest_base}/${dest_namespace}"
    cp "${source_base}/${target}" "${dest_base}/${dest_namespace}${target}"
  fi
}

notice Installing Spark
$(fetch_spark_tarball) | tar xzC $build_dir
mv $build_dir/spark-$spark_version-bin-hadoop$hadoop_version $build_dir/spark-home
mkdir -p $build_dir/bin
cautious_build_overlay bin/master-url
cautious_build_overlay bin/spark-master
cautious_build_overlay bin/spark-master-with-graceful-cycling
cautious_build_overlay bin/spark-shell
cautious_build_overlay bin/spark-submit
cautious_build_overlay bin/spark-worker
finish Installing Spark


notice Configuring Spark
rm -rf $build_dir/spark-home/lib/spark-examples-*
rm -rf $build_dir/spark-home/yarn
mkdir -p $build_dir/spark-home/conf
cautious_build_overlay conf/spark-env.sh spark-home/
mkdir -p $build_dir/conf
cautious_build_overlay conf/log4j.properties.erb
cautious_build_overlay conf/spark-defaults.conf.erb
finish Configuring Spark

start Installing s3n:// and s3a:// HDFS Support
mkdir -p $build_dir/spark-home/lib
curl -s -L https://s3.amazonaws.com/heroku-spark/libhadoop.so.1.0.0 > $build_dir/spark-home/lib/libhadoop.so
curl -s -L https://s3.amazonaws.com/heroku-spark/libhdfs.so.0.0.0 > $build_dir/spark-home/lib/libhdfs.so
curl -s -L https://s3.amazonaws.com/heroku-spark/$hadoop_aws_shade > $build_dir/spark-home/lib/hadoop-aws-shaded.jar
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/liblzo2.so.2.0.0 > $build_dir/spark-home/lib/liblzo2.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/libgplcompression.so.0.0.0 > $build_dir/spark-home/lib/libgplcompression.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/hadoop-lzo-0.4.20-SNAPSHOT.jar > $build_dir/spark-home/lib/hadoop-lzo.jar
finish Installing s3a:// HDFS Support



